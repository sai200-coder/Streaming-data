# Streaming Data Project using databricks


##   Real-Time Crypto Trading Data  (Databricks + Faker)
 Project Overview

This project demonstrates how to design and build a real-time data engineering  in Databricks using synthetic but realistic data generated with the Faker Python library.
The goal of the project is to simulate a crypto trading system â€” including customers, wallets, orders, trades, and price movements â€” and process it through Bronze â†’ Silver â†’ Gold layers following the Delta Lake architecture.

ðŸš€ Key Objectives

Generate structured, balanced, and relational fake data using the Faker module.

Build a real-time streaming data ingestion  in Databricks.

Design and implement Bronze, Silver, and Gold layers using Delta Lake.

Perform data cleaning, transformations, joins, and aggregations for analytics.

Create queryable and joinable data models for business insights.

ðŸ§© Data Model Overview
```yaml
Entity Relationships
CUSTOMERS (1) â”€â”€â”€< WALLETS (N)
WALLETS (1) â”€â”€â”€< ORDERS (N)
ORDERS (1) â”€â”€â”€< TRADES (N)
TRADES (1) â”€â”€â”€< PRICES (1)
```
```
Tables and Columns
Table	       Key	       Description
CUSTOMERS	customer_id	  Contains customer details such as name, email, phone, address, and KYC info.
WALLETS	  wallet_id,    FK â†’ customers.customer_id	Stores customer wallet balances and currency type.
ORDERS	   order_id,     FK â†’ wallets.wallet_id	Records buy/sell crypto orders linked to wallets and customers.
TRADES	   trade_id,     FK â†’ orders.order_id	Contains trade execution details including fees and value.
PRICES	   price_id,     FK â†’ trades.trade_id	Stores asset prices and timestamps for trade valuation.
```
ðŸ§  Data Generation (Faker)

The dataset is generated using Pythonâ€™s Faker library to simulate realistic trading activity.

Balanced and relational data:
Each generated record is connected across tables (no missing joins).

Independent JSON outputs:
Separate JSON files are created for each table (customers.json, wallets.json, etc.) and each execution generates fresh random data.

Example Record (Orders Table):

{
  "order_id": 1,
  "customer_id": 1,
  "wallet_id": 1,
  "order_type": "BUY",
  "asset": "BTC",
  "price": 48778.02,
  "quantity": 1.0921,
  "order_status": "COMPLETED",
  "created_at": "2025-10-29T01:41:05.199419+00:00",
  "updated_at": "2025-10-29T01:41:05.199421+00:00"
}

ðŸ—ï¸  Architecture
1ï¸âƒ£ Bronze Layer

Ingests streaming JSON files generated by Faker.

Raw, unprocessed data stored as Delta tables.

Handles real-time ingestion using readStream.

2ï¸âƒ£ Silver Layer

Cleans and standardizes data (removes duplicates, handles nulls).

Enforces relationships across all tables.

Performs initial joins and transformations.

Ensures timestamps and numeric types are properly formatted.

3ï¸âƒ£ Gold Layer

Joins all Silver tables into a unified analytical dataset.

Performs aggregations such as:

Customer wallet and trade summaries

Asset performance analytics

Daily trade value trends

Provides ready-to-query Delta tables for BI dashboards.

âš™ï¸ Technologies Used

Databricks (for Delta Lake, Spark SQL, and streaming pipelines)

Python & PySpark

Faker (for generating synthetic data)

Delta Lake (for data versioning and ACID transactions)

ðŸ’¡ Key Learnings

How to design a realistic relational data model for trading systems.

Building a Bronzeâ€“Silverâ€“Gold data layers using Delta architecture.

Managing data consistency and referential integrity in Faker-generated data.

Writing aggregation and analytical transformations in PySpark.

Understanding how to make data queryable, structured, and ready for insights.



 Author

Project by: Sai Mukhesh

Tech Stack: Databricks | PySpark | Delta Lake | Faker | JSON Streaming
Year: 2025
