{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c732ffd8-accd-4682-b458-1591f21d3540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Customers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335d5867-edec-487b-bc54-1300b08b3888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Read streaming JSON data from the raw users directory using Auto Loader\n",
    "df_users = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/stream1/stream1/poc_streaming/checkpoints/customers_schema\")  # Path to store schema\n",
    "        .option(\"mergeSchema\", \"true\")  # Enable schema evolution\n",
    "        .load(\"/Volumes/stream1/stream1/poc_streaming/customers/\")\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))  # Source directory\n",
    ")\n",
    "\n",
    "# Drop the _rescued_data column if present\n",
    "df_users = df_users.drop(\"_rescued_data\")\n",
    "\n",
    "# Write the streaming DataFrame to a Delta tabwwwwwle in append mode.\n",
    "# ERROR: partitionBy is not supported with toTable() in streaming write\n",
    "df_users.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/stream1/stream1/poc_streaming/checkpoints/customers_stream\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"stream1.stream1.customers_bronze\")  # Target Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04aba4d9-06f4-49d2-93ee-00543ac73b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Products table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c8f1d5-0f04-4a90-b39a-0d352f085325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read streaming JSON data from the raw products directory using Auto Loader\n",
    "df_products = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\",  \"/Volumes/stream1/stream1/poc_streaming/checkpoints/orders_schema\")  # Path to store schema\n",
    "        .option(\"mergeSchema\", \"true\")  # Enable schema evolution\n",
    "        .load(\"/Volumes/stream1/stream1/poc_streaming/orders/\")  # Source directory\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "# Drop the _rescued_data column if present\n",
    "df_products = df_products.drop(\"_rescued_data\")\n",
    "\n",
    "# Write the streaming DataFrame to a Delta table in append mode\n",
    "df_products.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\",  \"/Volumes/stream1/stream1/poc_streaming/checkpoints/orders_stream\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"stream1.stream1.orders_bronze\")  # Target Delta table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0187e8f2-f3f8-41b2-ac7b-1c8c33f79a2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Product table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87452302-e6bf-4b0a-93f7-2ab3d50c3fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read streaming JSON data from the raw payments directory using Auto Loader\n",
    "df_price = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/stream1/stream1/poc_streaming/checkpoints/prices_schema\")  # Path to store schema\n",
    "        .option(\"mergeSchema\", \"true\")  # Enable schema evolution\n",
    "        .load(\"/Volumes/stream1/stream1/poc_streaming/prices/\")  # Source directory\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "# Drop the _rescued_data column if present\n",
    "df_price = df_price.drop(\"_rescued_data\")\n",
    "\n",
    "# Write the streaming DataFrame to a Delta table in append mode\n",
    "df_price.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\",\"/Volumes/stream1/stream1/poc_streaming/checkpoints/prices_stream\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"stream1.stream1.prices_bronze\")  # Target Delta table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a149402-b54d-4e7b-b87a-670543d9abbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Trades table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d622b4-9703-41b6-8982-819b38ffa35b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read streaming JSON data from the raw trades directory using Auto Loader\n",
    "df_trades = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/stream1/stream1/poc_streaming/checkpoints/trades_schema\")  # Path to store schema\n",
    "        .option(\"mergeSchema\", \"true\")  # Enable schema evolution\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Infer new fields automatically\n",
    "        .load(\"/Volumes/stream1/stream1/poc_streaming/trades/\")  # Source directory\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "# Drop the _rescued_data column if present\n",
    "df_trades = df_trades.drop(\"_rescued_data\")\n",
    "\n",
    "# Write the streaming DataFrame to a Delta table in append mode\n",
    "df_trades.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/stream1/stream1/poc_streaming/checkpoints/trades_stream\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"stream1.stream1.trades_bronze\")  # Target Delta table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678f7770-86cb-4ad5-8043-c31900669e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Wallets table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff93b93-2bc7-407f-8a34-7b9be4e47dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read streaming JSON data from the raw wallets directory using Auto Loader\n",
    "df_wallets = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/stream1/stream1/poc_streaming/checkpoints/wallets_schema\")  # Path to store schema\n",
    "        .option(\"mergeSchema\", \"true\")  # Enable schema evolution\n",
    "        .load(\"/Volumes/stream1/stream1/poc_streaming/wallets/\")  # Source directory\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "# Drop the _rescued_data column if present\n",
    "df_wallets = df_wallets.drop(\"_rescued_data\")\n",
    "\n",
    "# Write the streaming DataFrame to a Delta table in append mode\n",
    "df_wallets.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/stream1/stream1/poc_streaming/checkpoints/wallets_stream\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"stream1.stream1.wallets_bronze\")  # Target Delta table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c4fbfb-1226-4f05-9bb9-866382654156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"stream1.stream1.customers_bronze\",\n",
    "    \"stream1.stream1.orders_bronze\",\n",
    "    \"stream1.stream1.prices_bronze\",\n",
    "    \"stream1.stream1.trades_bronze\",\n",
    "    \"stream1.stream1.wallets_bronze\"\n",
    "]\n",
    "\n",
    "counts = []\n",
    "for table in tables:\n",
    "    count = spark.table(table).count()\n",
    "    counts.append((table, count))\n",
    "\n",
    "df_counts = spark.createDataFrame(counts, [\"table_name\", \"record_count\"])\n",
    "display(df_counts)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6394031457219905,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
